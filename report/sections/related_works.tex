\section{Related works} \label{related_works}

	We have viewed different state-of-the-art papers. 
	In \cite{article1}, researchers used Google's Inception v3 CNN architecture pretrained on the 2014 ImageNet Challenge. They then removed the final classification layer from the network and retrained it with their dataset, fine-tuning the parameters across all layers. During training they resized each image to 299x299 pixels in order to made it compatible with the original dimensions of the Inception v3 network architecture. All layers of the network were fine-tuned using the same global learning rate and RMSProp optimizer. They performed their experiments on a 129.000 images dataset, created from a combination of different datasets. They obtained 72.1\% overall accuracy training their model on 757 classes. To create these training classes they used a taxonomy of skin disease and a partitioning algorithm that maps diseases into training classes. We have tried to obtain these data, but they were protected by the Stanford Hospital.
	
	\smallskip
	
	In \cite{article3}, researchers studied the effectiveness and capability of different pre-trained state-of-the-art CNN architectures (DenseNet 201, ResNet 152, Inception v3, InceptionResNet v2). All the models they used were pre-trained on the 2014 ImageNet Challenge. They changed the classification part of these models with a custom classifier and they retrained them across all the layers using different hyperparameters depending on the specific network architecture. They trained these models on a dataset of 10.135 dermoscopy skin images composed by the combination of HAM10000 and PH2\cite{ph2} datasets.  The aim of their project was to compare the ability of deep learning with the performances of highly trained dermatologists. Overall, the mean results show that all deep learning models outperformed dermatologists (at least 11\%). The best ROC AUC values for melanoma and basal cell carcinoma are 94.40\% (ResNet 152) and 99.30\% (DenseNet 201) versus 82.26\% and 88.82\% of dermatologists, respectively.
	
	\smallskip

	In \cite{article2}, researchers used a CNN model built from scratch. The proposed model architecture consists on a sequence of alternating Conv2D and MaxPooling2D layers that form the core building blocks of modern CNNs. Then, they putted a batch normalization layer after each ReLu activation. They used this model to perform a binary classification task, in fact the first convolutional layer takes in 224x224 skin lesion images and the last dense layer contains a single unit with sigmoid activation in order to output the resulting classes (benign and malignant). They trained their model on the PHDB melanoma dataset, created by their own from a combination of open access datasets. They obtained an accuracy of the 86\% on this dataset and regularization techniques such as dropout and data augmentation techniques were heavily relied upon to combat the overfitting problem.
	
	\smallskip
	
	After reading these papers we decided firstly to try the power of transfer learning on our dataset. Given the big amount of data in HAM10000 and the high difference between the images in our dataset and the images included in the ImageNet Challenge, the best transfer learning strategy was to train the pre-trained model across all the layers, but we did not have enough resources for it. So, we froze all the layers except for the classification part of the network and we trained the model. This technique gave us bad performances compare to the training of a simple CNN model built from scratch. So, we decided to use an approach that is more similar to the method explained in \cite{article2} than the other papers.
	